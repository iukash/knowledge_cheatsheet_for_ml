{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d8be63-5912-4b7c-941b-c9ddae530ae8",
   "metadata": {},
   "source": [
    "## Практическое задание №3\n",
    "### Метод кросс-энтропии с глубоким обучением (со средой gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "572c8a3d-6778-4fbf-869e-cc6705349988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4fda487-bb68-4365-b48f-d91a9f8a8faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_dim = 4\n",
    "action_n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b93bcfe2-8427-41f6-bc90-52ee028169e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 mean total reward: 19.75\n",
      "iteration: 1 mean total reward: 19.9\n",
      "iteration: 2 mean total reward: 25.65\n",
      "iteration: 3 mean total reward: 26.1\n",
      "iteration: 4 mean total reward: 33.8\n",
      "iteration: 5 mean total reward: 34.7\n",
      "iteration: 6 mean total reward: 42.55\n",
      "iteration: 7 mean total reward: 41.35\n",
      "iteration: 8 mean total reward: 41.95\n",
      "iteration: 9 mean total reward: 54.25\n",
      "iteration: 10 mean total reward: 44.0\n",
      "iteration: 11 mean total reward: 52.8\n",
      "iteration: 12 mean total reward: 65.2\n",
      "iteration: 13 mean total reward: 47.0\n",
      "iteration: 14 mean total reward: 58.25\n",
      "iteration: 15 mean total reward: 66.35\n",
      "iteration: 16 mean total reward: 61.35\n",
      "iteration: 17 mean total reward: 74.75\n",
      "iteration: 18 mean total reward: 90.8\n",
      "iteration: 19 mean total reward: 77.1\n",
      "iteration: 20 mean total reward: 69.45\n",
      "iteration: 21 mean total reward: 75.0\n",
      "iteration: 22 mean total reward: 71.55\n",
      "iteration: 23 mean total reward: 80.8\n",
      "iteration: 24 mean total reward: 71.55\n",
      "iteration: 25 mean total reward: 76.2\n",
      "iteration: 26 mean total reward: 71.65\n",
      "iteration: 27 mean total reward: 118.75\n",
      "iteration: 28 mean total reward: 114.5\n",
      "iteration: 29 mean total reward: 112.5\n",
      "iteration: 30 mean total reward: 126.55\n",
      "iteration: 31 mean total reward: 115.65\n",
      "iteration: 32 mean total reward: 93.3\n",
      "iteration: 33 mean total reward: 96.95\n",
      "iteration: 34 mean total reward: 73.45\n",
      "iteration: 35 mean total reward: 79.25\n",
      "iteration: 36 mean total reward: 71.15\n",
      "iteration: 37 mean total reward: 77.85\n",
      "iteration: 38 mean total reward: 87.45\n",
      "iteration: 39 mean total reward: 114.85\n",
      "iteration: 40 mean total reward: 130.1\n",
      "iteration: 41 mean total reward: 166.45\n",
      "iteration: 42 mean total reward: 154.25\n",
      "iteration: 43 mean total reward: 156.75\n",
      "iteration: 44 mean total reward: 131.55\n",
      "iteration: 45 mean total reward: 113.25\n",
      "iteration: 46 mean total reward: 117.95\n",
      "iteration: 47 mean total reward: 109.25\n",
      "iteration: 48 mean total reward: 102.4\n",
      "iteration: 49 mean total reward: 104.55\n",
      "iteration: 50 mean total reward: 141.45\n",
      "iteration: 51 mean total reward: 144.8\n",
      "iteration: 52 mean total reward: 181.55\n",
      "iteration: 53 mean total reward: 199.2\n",
      "iteration: 54 mean total reward: 214.8\n",
      "iteration: 55 mean total reward: 237.7\n",
      "iteration: 56 mean total reward: 170.75\n",
      "iteration: 57 mean total reward: 207.9\n",
      "iteration: 58 mean total reward: 207.0\n",
      "iteration: 59 mean total reward: 237.15\n",
      "iteration: 60 mean total reward: 263.6\n",
      "iteration: 61 mean total reward: 302.4\n",
      "iteration: 62 mean total reward: 322.75\n",
      "iteration: 63 mean total reward: 405.45\n",
      "iteration: 64 mean total reward: 358.6\n",
      "iteration: 65 mean total reward: 340.95\n",
      "iteration: 66 mean total reward: 321.8\n",
      "iteration: 67 mean total reward: 378.45\n",
      "iteration: 68 mean total reward: 396.85\n",
      "iteration: 69 mean total reward: 439.25\n",
      "iteration: 70 mean total reward: 460.2\n",
      "iteration: 71 mean total reward: 465.9\n",
      "iteration: 72 mean total reward: 466.9\n",
      "iteration: 73 mean total reward: 449.75\n",
      "iteration: 74 mean total reward: 457.25\n",
      "iteration: 75 mean total reward: 482.5\n",
      "iteration: 76 mean total reward: 415.05\n",
      "iteration: 77 mean total reward: 495.65\n",
      "iteration: 78 mean total reward: 471.85\n",
      "iteration: 79 mean total reward: 467.55\n",
      "iteration: 80 mean total reward: 479.1\n",
      "iteration: 81 mean total reward: 468.25\n",
      "iteration: 82 mean total reward: 473.1\n",
      "iteration: 83 mean total reward: 482.0\n",
      "iteration: 84 mean total reward: 486.0\n",
      "iteration: 85 mean total reward: 468.35\n",
      "iteration: 86 mean total reward: 468.7\n",
      "iteration: 87 mean total reward: 480.9\n",
      "iteration: 88 mean total reward: 475.4\n",
      "iteration: 89 mean total reward: 472.15\n",
      "iteration: 90 mean total reward: 491.85\n",
      "iteration: 91 mean total reward: 458.6\n",
      "iteration: 92 mean total reward: 462.85\n",
      "iteration: 93 mean total reward: 470.25\n",
      "iteration: 94 mean total reward: 451.05\n",
      "iteration: 95 mean total reward: 452.4\n",
      "iteration: 96 mean total reward: 426.0\n",
      "iteration: 97 mean total reward: 413.65\n",
      "iteration: 98 mean total reward: 447.8\n",
      "iteration: 99 mean total reward: 447.5\n"
     ]
    }
   ],
   "source": [
    "# случайный агент\n",
    "class CEMDL(nn.Module):\n",
    "    def __init__(self, state_dim, action_n):\n",
    "        super().__init__()\n",
    "        self.action_n = action_n\n",
    "        self.state_dim = state_dim\n",
    "        self.network = nn.Sequential(nn.Linear(self.state_dim, 100),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(100, self.action_n))\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.optimazer = torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_):\n",
    "        return self.network(input_)\n",
    "\n",
    "    def fit(self, elite_trajectories):\n",
    "        elite_states = []\n",
    "        elite_actions = []\n",
    "        for elite_trajectoty in elite_trajectories:\n",
    "            for state, action in zip(elite_trajectoty['states'], elite_trajectoty['actions']):\n",
    "                elite_states.append(state)\n",
    "                elite_actions.append(action)\n",
    "\n",
    "        elite_states = torch.FloatTensor(elite_states)\n",
    "        elite_actions = torch.LongTensor(elite_actions)\n",
    "        pred_actions = self.forward(elite_states)        \n",
    "        loss = self.loss(pred_actions, elite_actions)\n",
    "        loss.backward()\n",
    "        self.optimazer.step()\n",
    "        self.optimazer.zero_grad()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        logits = self.forward(torch.FloatTensor(state))\n",
    "        probs = self.softmax(logits).detach().numpy()\n",
    "        action = np.random.choice(self.action_n, p=probs)\n",
    "        return action\n",
    "        \n",
    "# получить траекторию\n",
    "def get_trajectory(env, agent, max_len=500, vizualize=False):\n",
    "    trajectory = {'states': [], 'actions': [], 'reward': 0}\n",
    "\n",
    "    # инициализация начального состояния\n",
    "    state = env.reset()\n",
    "    trajectory['states'].append(state)\n",
    "\n",
    "    # стратегия действия агента\n",
    "    for _ in range(max_len):\n",
    "        action = agent.get_action(state)\n",
    "        trajectory['actions'].append(action)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        trajectory['reward'] += reward\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if vizualize:\n",
    "            time.sleep(0.01)\n",
    "            env.render()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        trajectory['states'].append(state)\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "# инициализация агента\n",
    "agent = CEMDL(state_dim, action_n)\n",
    "q_param = 0.8\n",
    "iteration_n = 100\n",
    "trajectory_n = 20\n",
    "\n",
    "for iteration in range(iteration_n):\n",
    "    # оценка политики\n",
    "    trajectories = [get_trajectory(env, agent, max_len=500, vizualize=False) for _ in range(trajectory_n)]\n",
    "    mean_total_reward = np.mean([trajectory['reward'] for trajectory in trajectories])\n",
    "    print('iteration:', iteration, 'mean total reward:', mean_total_reward)\n",
    "\n",
    "    # улучшение политики\n",
    "    quantile = np.quantile(total_reward, q_param)\n",
    "    elite_trajectories = []\n",
    "    for trajectory in trajectories:\n",
    "        total_reward = trajectory['reward']\n",
    "        if total_reward > quantile:\n",
    "            elite_trajectories.append(trajectory)\n",
    "    if len(elite_trajectories) > 0:\n",
    "        agent.fit(elite_trajectories)\n",
    "\n",
    "#get_trajectory(env, agent, 1, vizualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b61c6f5-54db-45fc-a627-5aeb1bbd5984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
