{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c7f390c-7748-430d-aa70-922d3f15ebf0",
   "metadata": {},
   "source": [
    "## Обучение с подкреплением\n",
    "Обучение с подкреплением - это обучение модели действиям, которые максимизируют численный сигнал-вознаграждение. В большинстве сложных задач действия необходимо рассматривать действия не только как влияющие на непосредственное вознаграждение, но и на следующую ситуацию, а значит на все последующие вознаграждения. Задачу обучения с подкреплением можно формализовать как задачу оптимального управления марковским процессом принятия решений. Обучающийся агент должен уметь в какой-то степени воспринимать состояние среды и предпринимать действия, изменяющие его состояние, с целью достижения поставленной цели. Важная черта обучения с подкреплением – явное рассмотрение целостной проблемы целеустремленного агента, взаимодействующего с неопределенной окружающей средой. При этом под агентом понимается программный математический модуль, который будет принимать решения, выполнять действия и на основании ответов среды обучаться. Сторона, генерирующая вознаграждения за действия агента, называется окружающей средой. Генерируемые средой вознаграждения определяют цель агента – максимизация полного вознаграждения (сумма вознаграждений за длительный период времени). Для достижения этой цели агенту необходимо сформировать оптимальную стратегию своих действий, основанную на итерационном выполнении действий и сохранении вектора полученных вознаграждений в ответ на определенные события. Формирование оптимальной стратегии и есть основная задача обучения с подкреплением, поскольку ее достаточно для определения наилучшего поведения агента в текущих условиях окружающей среды. Инструментом для решения задачи поиска оптимальной стратегии выступает функция ценности, которая оценивает возможность получить вознаграждение не только сейчас, но и в длительной перспективе.\n",
    "\n",
    "### Математическая постановка задачи\n",
    "Поскольку поведение агента в среде можно описать **марковским процессом принятия решения** введем **необходимые определения**:\n",
    "\n",
    "* $S$ - множество состояний\n",
    "* $A$ - множество действия\n",
    "* $Pfunc$ - функция возвращающая веростность перехода в следующее состояние при выборе определенного действия в определенном\n",
    "$Pfunc(s^{'}|s, a) = P[S_{t+1} = s^{'}| S_t = s, A_t = a]$\n",
    "* $Pfunc_0$ - функция инициализации начального (нулевого) состояния\n",
    "* $Rfunc$ - функция выдачи награды $Rfunc(s, a) = R_t \\iff P[R_t|S_t = s, A_t = a] = 1$\n",
    "* $\\gamma$ - коэффициент дисконтирования (угасания ценности последующих наград относительно награды на следующем шаге)\n",
    "\n",
    "и следующие марковские свойства:\n",
    "\n",
    "$P[S_{t+1}|S_t, A_t] = P[S_{t+1}|S_0, A_0, S_1, A_1...S_t, A_t]$ - вероятность перехода агента в следующее состояние в которое агент перемещается после действия зависит только от текущего состояния и действия (не зависит от предыдущих состояний, действий, полученных наград и т.д.)\n",
    "\n",
    "$P[R_t|S_t, A_t] = P[R_t|S_0, A_0, S_1, A_1...S_t, A_t] = 1$ - получаемая награда агента зависит от текущего состояния и действия и детерминирована, т.е. награда является постоянной величиной и за определенный переход в следующее состояние мы всегда будем получать одинаковое вознаграждение.\n",
    "\n",
    "**Цель агента** - максимизация суммы с наград с учетом коэффициента дисконтирования $G = \\sum^{\\infty}_{t=0}\\gamma^tR_t \\rightarrow max$\n",
    "\n",
    "или для марковского процесса с финальными состояниями $G = \\sum^{T}_{t=0}\\gamma^tR_t \\rightarrow max$, где $S_T \\in S_F$\n",
    "\n",
    "При этом мы можем преобразовывать задачу от одной к другой, для преобразования задачи с финальными состояними к общей принимаем множество финальных состояний пустым, обратную задачу решаем определением состояний (по сути финальных) с вероятностью перехода в него же равной единице и нулевой наградой за такой переход.\n",
    "\n",
    "### Интерфейс OpenAI Gym Interface\n",
    "Функции:\n",
    "* initial_state = env.reset()\n",
    "    * unitial_state - инициализация начального состояния $S_0 \\sim Pfunc_0$\n",
    "    * env.state = initial_state\n",
    "* next_state, reward, done, info = env.step(action)\n",
    "    * action - текущее действие $A_t$\n",
    "    * next_state - следующее состояние $S_{t+1} \\sim Pfunc(S_{t+1}|S_t, A_t)$\n",
    "    * reward - текущая награда $R_t = Rfunc(S_t, A_t)$\n",
    "    * done - при true достигнуто финальное состояние $S_{t+1} \\in S_F$\n",
    "    * info - дополнительная информация\n",
    "    * env.state = next_state\n",
    " \n",
    "### Политика\n",
    "Политика - это последовательность действий $\\pi: S \\rightarrow A$\n",
    "* имеем множество политик $\\pi$\n",
    "* агент инициализирует начальное состояние $S_0 \\sim Pfunc_0$\n",
    "* совершает действие в соответствии с политикой $A_0 = \\pi(S_0)$\n",
    "* получает награду $R_0 = Rfunc(S_0, A_0)$ и переходит в следующее состояние $S_1 \\sim Pfunc(S_1|S_0, A_0)$\n",
    "* совершает следующее действие $A_1 = \\pi(S_1)$\n",
    "* получает награду $R_1 = Rfunc(S_1, A_1)$ и так далее\n",
    "* ...\n",
    "* получаем конечную сумму наград (ценность) $G = \\sum^{\\infty}_{t=0}\\gamma^tR_t$\n",
    "\n",
    "Соответственно задача обучения с подкреплением сводится к выбору оптимальной стратегии максимизирующей математическое ожидание ценности $\\mathbb{E}_{\\pi}[G] \\rightarrow \\underset{\\pi}{max}$\n",
    "\n",
    "**Стохастическая политика** - политика при котором в состояние выдается не действие, а вероятностное распределение действий $\\pi(a|s) \\in [0, 1]$\n",
    "\n",
    "При действии в соответствии с политикой получаем **траекторию** $\\tau = \\{S_0, A_0, S_1, A_1, S_2, A_2 ...\\}$\n",
    "\n",
    "Соответственно вероятность получить траекторию $\\tau$ при политике $\\pi$ является произведением всех вероятностей перехода $\\mathbb{P}(\\tau, \\pi) = \\prod_{t=0}^{\\infty} \\pi(A_t|S_t)Pfunc(S_{t+1}|S_t, A_t)$\n",
    "\n",
    "А математическое ожидание ценности равно сумме ценностей с учетом вероятностей траекторий $\\mathbb{E}_{\\pi}[G] = \\sum_{\\tau} G(\\tau)\\mathbb{P}(\\tau, \\pi)$ или интрегралу при непрерывных траекториях $\\mathbb{E}_{\\pi}[G] = \\int_{\\tau} G(\\tau)\\mathbb{P}(\\tau, \\pi)$\n",
    "\n",
    "### Расчет матожидания ценности $\\mathbb{E}_{\\pi}[G]$\n",
    "Ценность приблизительно равна сумме ценностей семплированный K стратегий при K стремящемся к $\\infty$\n",
    "$$\\mathbb{E}_{\\pi}[G] \\approx \\frac{1}{K} \\sum_{k=1}^K G(\\tau_k), \\tau_k \\sim \\{P, P_0, \\pi\\}$$\n",
    "\n",
    "Соответственно расчет ценности представляет собой достаточно сложный процесс вычисления, а с учетом оптимизации по стратегиям становится еще сложнее. Для решения поставленной задачи используются различные алгоритмы обучения с подкреплением, которые рассмотрены отдельными файлами."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d92d96-16d6-4d36-8300-c0b4d26935b2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
