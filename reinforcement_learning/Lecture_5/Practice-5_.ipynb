{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "\n",
    "Задаем структуру аппроксимации $Q^\\theta$, начальные вектор параметров $\\theta$, вероятность исследования среды $\\varepsilon = 1$.\n",
    "\n",
    "Для каждого эпизода $k$ делаем:\n",
    "\n",
    "Пока эпизод не закончен делаем:\n",
    "\n",
    "- Находясь в состоянии $S_t$ совершаем действие $A_t \\sim \\pi(\\cdot|S_t)$, где $\\pi = \\varepsilon\\text{-greedy}(Q^\\theta)$, получаем награду $R_t$  переходим в состояние $S_{t+1}$. Сохраняем $(S_t,A_t,R_t,S_{t+1}) \\rightarrow Memory$\n",
    "\n",
    "\n",
    "- Берем $\\{(s_i,a_i,r_i,s'_i)\\}_{i=1}^{n} \\leftarrow Memory$, определяем целевые значения\n",
    "\n",
    "$$\n",
    "y_i =\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "r_i, &\\text{ если } s'_i\\text{ -терминальное},\\\\[0.0cm]\n",
    " r_i + \\gamma \\max\\limits_{a'} Q^\\theta(s'_i,a'), &\\text{ иначе}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "функцию потерь $Loss(\\theta) = \\frac{1}{n}\\sum\\limits_{i=1}^n \\big(y_i - Q^\\theta(s_i,a_i)\\big)^2$\n",
    "и обновляем вектор параметров\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta Loss(\\theta)\n",
    "$$\n",
    "\n",
    "- Уменьшаем $\\varepsilon$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T12:05:45.405043Z",
     "start_time": "2020-11-26T12:05:45.381352Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Qfunction(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(state_dim, 64)\n",
    "        self.linear_2 = nn.Linear(64, 64)\n",
    "        self.linear_3 = nn.Linear(64, action_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, states):\n",
    "        hidden = self.linear_1(states)\n",
    "        hidden = self.activation(hidden)\n",
    "        hidden = self.linear_2(hidden)\n",
    "        hidden = self.activation(hidden)\n",
    "        actions = self.linear_3(hidden)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T12:07:05.971803Z",
     "start_time": "2020-11-26T12:05:45.804040Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-3, batch_size=64, epsilon_decrease=0.01, epilon_min=0.01):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.q_function = Qfunction(self.state_dim, self.action_dim)\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decrease = epsilon_decrease\n",
    "        self.epilon_min = epilon_min\n",
    "        self.memory = []\n",
    "        self.optimzaer = torch.optim.Adam(self.q_function.parameters(), lr=lr)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        q_values = self.q_function(torch.FloatTensor(state))\n",
    "        argmax_action = torch.argmax(q_values)\n",
    "        probs = self.epsilon * np.ones(self.action_dim) / self.action_dim\n",
    "        probs[argmax_action] += 1 - self.epsilon\n",
    "        action = np.random.choice(np.arange(self.action_dim), p=probs)\n",
    "        return action\n",
    "    \n",
    "    def fit(self, state, action, reward, done, next_state):\n",
    "        self.memory.append([state, action, reward, int(done), next_state])\n",
    "\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "            states, actions, rewards, dones, next_states = map(torch.tensor, list(zip(*batch)))\n",
    "    \n",
    "            targets = rewards + self.gamma * (1 - dones) * torch.max(self.q_function(next_states), dim=1).values\n",
    "            q_values = self.q_function(states)[torch.arange(self.batch_size), actions]\n",
    "            \n",
    "            loss = torch.mean((q_values - targets.detach()) ** 2)\n",
    "            loss.backward()\n",
    "            self.optimzaer.step()\n",
    "            self.optimzaer.zero_grad()\n",
    "            \n",
    "            if self.epsilon > self.epilon_min:\n",
    "                self.epsilon -= self.epsilon_decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, total_reward: 13.0\n",
      "episode: 1, total_reward: 11.0\n",
      "episode: 2, total_reward: 20.0\n",
      "episode: 3, total_reward: 13.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5006/3928126314.py:27: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  states, actions, rewards, dones, next_states = map(torch.tensor, list(zip(*batch)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4, total_reward: 15.0\n",
      "episode: 5, total_reward: 11.0\n",
      "episode: 6, total_reward: 9.0\n",
      "episode: 7, total_reward: 19.0\n",
      "episode: 8, total_reward: 10.0\n",
      "episode: 9, total_reward: 9.0\n",
      "episode: 10, total_reward: 16.0\n",
      "episode: 11, total_reward: 10.0\n",
      "episode: 12, total_reward: 10.0\n",
      "episode: 13, total_reward: 10.0\n",
      "episode: 14, total_reward: 9.0\n",
      "episode: 15, total_reward: 9.0\n",
      "episode: 16, total_reward: 8.0\n",
      "episode: 17, total_reward: 10.0\n",
      "episode: 18, total_reward: 10.0\n",
      "episode: 19, total_reward: 11.0\n",
      "episode: 20, total_reward: 11.0\n",
      "episode: 21, total_reward: 10.0\n",
      "episode: 22, total_reward: 11.0\n",
      "episode: 23, total_reward: 19.0\n",
      "episode: 24, total_reward: 15.0\n",
      "episode: 25, total_reward: 14.0\n",
      "episode: 26, total_reward: 15.0\n",
      "episode: 27, total_reward: 15.0\n",
      "episode: 28, total_reward: 14.0\n",
      "episode: 29, total_reward: 33.0\n",
      "episode: 30, total_reward: 51.0\n",
      "episode: 31, total_reward: 44.0\n",
      "episode: 32, total_reward: 62.0\n",
      "episode: 33, total_reward: 17.0\n",
      "episode: 34, total_reward: 31.0\n",
      "episode: 35, total_reward: 37.0\n",
      "episode: 36, total_reward: 16.0\n",
      "episode: 37, total_reward: 28.0\n",
      "episode: 38, total_reward: 26.0\n",
      "episode: 39, total_reward: 52.0\n",
      "episode: 40, total_reward: 42.0\n",
      "episode: 41, total_reward: 106.0\n",
      "episode: 42, total_reward: 56.0\n",
      "episode: 43, total_reward: 125.0\n",
      "episode: 44, total_reward: 80.0\n",
      "episode: 45, total_reward: 198.0\n",
      "episode: 46, total_reward: 189.0\n",
      "episode: 47, total_reward: 260.0\n",
      "episode: 48, total_reward: 223.0\n",
      "episode: 49, total_reward: 191.0\n",
      "episode: 50, total_reward: 179.0\n",
      "episode: 51, total_reward: 196.0\n",
      "episode: 52, total_reward: 206.0\n",
      "episode: 53, total_reward: 316.0\n",
      "episode: 54, total_reward: 373.0\n",
      "episode: 55, total_reward: 245.0\n",
      "episode: 56, total_reward: 168.0\n",
      "episode: 57, total_reward: 193.0\n",
      "episode: 58, total_reward: 216.0\n",
      "episode: 59, total_reward: 163.0\n",
      "episode: 60, total_reward: 195.0\n",
      "episode: 61, total_reward: 191.0\n",
      "episode: 62, total_reward: 175.0\n",
      "episode: 63, total_reward: 155.0\n",
      "episode: 64, total_reward: 162.0\n",
      "episode: 65, total_reward: 191.0\n",
      "episode: 66, total_reward: 224.0\n",
      "episode: 67, total_reward: 200.0\n",
      "episode: 68, total_reward: 216.0\n",
      "episode: 69, total_reward: 186.0\n",
      "episode: 70, total_reward: 185.0\n",
      "episode: 71, total_reward: 203.0\n",
      "episode: 72, total_reward: 500.0\n",
      "episode: 73, total_reward: 214.0\n",
      "episode: 74, total_reward: 175.0\n",
      "episode: 75, total_reward: 186.0\n",
      "episode: 76, total_reward: 184.0\n",
      "episode: 77, total_reward: 190.0\n",
      "episode: 78, total_reward: 144.0\n",
      "episode: 79, total_reward: 150.0\n",
      "episode: 80, total_reward: 135.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = DQN(state_dim, action_dim)\n",
    "\n",
    "episode_n = 100\n",
    "t_max = 500\n",
    "\n",
    "for episode in range(episode_n):\n",
    "    total_reward = 0\n",
    "\n",
    "    state = env.reset()\n",
    "    for t in range(t_max):\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        \n",
    "        agent.fit(state, action, reward, done, next_state)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f'episode: {episode}, total_reward: {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
