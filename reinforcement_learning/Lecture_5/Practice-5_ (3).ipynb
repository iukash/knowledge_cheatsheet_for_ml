{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "\n",
    "Задаем структуру аппроксимации $Q^\\theta$, начальные вектор параметров $\\theta$, вероятность исследования среды $\\varepsilon = 1$.\n",
    "\n",
    "Для каждого эпизода $k$ делаем:\n",
    "\n",
    "Пока эпизод не закончен делаем:\n",
    "\n",
    "- Находясь в состоянии $S_t$ совершаем действие $A_t \\sim \\pi(\\cdot|S_t)$, где $\\pi = \\varepsilon\\text{-greedy}(Q^\\theta)$, получаем награду $R_t$  переходим в состояние $S_{t+1}$. Сохраняем $(S_t,A_t,R_t,S_{t+1}) \\rightarrow Memory$\n",
    "\n",
    "\n",
    "- Берем $\\{(s_i,a_i,r_i,s'_i)\\}_{i=1}^{n} \\leftarrow Memory$, определяем целевые значения\n",
    "\n",
    "$$\n",
    "y_i =\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "r_i, &\\text{ если } s'_i\\text{ -терминальное},\\\\[0.0cm]\n",
    " r_i + \\gamma \\max\\limits_{a'} Q^\\theta(s'_i,a'), &\\text{ иначе}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "функцию потерь $Loss(\\theta) = \\frac{1}{n}\\sum\\limits_{i=1}^n \\big(y_i - Q^\\theta(s_i,a_i)\\big)^2$\n",
    "и обновляем вектор параметров\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta Loss(\\theta)\n",
    "$$\n",
    "\n",
    "- Уменьшаем $\\varepsilon$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T12:05:45.405043Z",
     "start_time": "2020-11-26T12:05:45.381352Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Qfunction(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(state_dim, 64)\n",
    "        self.linear_2 = nn.Linear(64, 64)\n",
    "        self.linear_3 = nn.Linear(64, action_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, states):\n",
    "        hidden = self.linear_1(states)\n",
    "        hidden = self.activation(hidden)\n",
    "        hidden = self.linear_2(hidden)\n",
    "        hidden = self.activation(hidden)\n",
    "        actions = self.linear_3(hidden)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T12:07:05.971803Z",
     "start_time": "2020-11-26T12:05:45.804040Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-3, batch_size=64, epsilon_decrease=0.01, epilon_min=0.01):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.q_function = Qfunction(self.state_dim, self.action_dim)\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decrease = epsilon_decrease\n",
    "        self.epilon_min = epilon_min\n",
    "        self.memory = []\n",
    "        self.optimzaer = torch.optim.Adam(self.q_function.parameters(), lr=lr)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        q_values = self.q_function(torch.FloatTensor(state))\n",
    "        argmax_action = torch.argmax(q_values)\n",
    "        probs = self.epsilon * np.ones(self.action_dim) / self.action_dim\n",
    "        probs[argmax_action] += 1 - self.epsilon\n",
    "        action = np.random.choice(np.arange(self.action_dim), p=probs)\n",
    "        return action\n",
    "    \n",
    "    def fit(self, state, action, reward, done, next_state):\n",
    "        self.memory.append([state, action, reward, int(done), next_state])\n",
    "\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "            states, actions, rewards, dones, next_states = map(torch.tensor, list(zip(*batch)))\n",
    "    \n",
    "            targets = rewards + self.gamma * (1 - dones) * torch.max(self.q_function(next_states), dim=1).values\n",
    "            q_values = self.q_function(states)[torch.arange(self.batch_size), actions]\n",
    "            \n",
    "            loss = torch.mean((q_values - targets.detach()) ** 2)\n",
    "            loss.backward()\n",
    "            self.optimzaer.step()\n",
    "            self.optimzaer.zero_grad()\n",
    "            \n",
    "            if self.epsilon > self.epilon_min:\n",
    "                self.epsilon -= self.epsilon_decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
      "/tmp/ipykernel_12629/3928126314.py:27: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  states, actions, rewards, dones, next_states = map(torch.tensor, list(zip(*batch)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, total_reward: -372.7308908563593\n",
      "episode: 1, total_reward: -410.30624221864247\n",
      "episode: 2, total_reward: -575.7010536447261\n",
      "episode: 3, total_reward: -274.5813217697789\n",
      "episode: 4, total_reward: -498.7260789912964\n",
      "episode: 5, total_reward: -434.8388794319086\n",
      "episode: 6, total_reward: -528.6408442983131\n",
      "episode: 7, total_reward: -70.69198215952044\n",
      "episode: 8, total_reward: -334.0660163037848\n",
      "episode: 9, total_reward: -170.2907502089159\n",
      "episode: 10, total_reward: -28.1973463636361\n",
      "episode: 11, total_reward: 7.138558768088785\n",
      "episode: 12, total_reward: -62.49648311685253\n",
      "episode: 13, total_reward: 29.695540317176167\n",
      "episode: 14, total_reward: 20.903753986497055\n",
      "episode: 15, total_reward: 53.94829781036837\n",
      "episode: 16, total_reward: -19.487856601458272\n",
      "episode: 17, total_reward: 28.047639926790882\n",
      "episode: 18, total_reward: -3.0649774181693097\n",
      "episode: 19, total_reward: 31.425596567054505\n",
      "episode: 20, total_reward: 10.06711438031716\n",
      "episode: 21, total_reward: -24.76374185847432\n",
      "episode: 22, total_reward: 5.373818813760662\n",
      "episode: 23, total_reward: 26.78944561494835\n",
      "episode: 24, total_reward: 57.368538511319485\n",
      "episode: 25, total_reward: 15.076739116456084\n",
      "episode: 26, total_reward: 21.14934482845805\n",
      "episode: 27, total_reward: 13.133080245522205\n",
      "episode: 28, total_reward: 36.489853995056606\n",
      "episode: 29, total_reward: -7.811869527405135\n",
      "episode: 30, total_reward: -15.350944279106928\n",
      "episode: 31, total_reward: 8.113324745829532\n",
      "episode: 32, total_reward: -51.3887011037452\n",
      "episode: 33, total_reward: -244.31092898522652\n",
      "episode: 34, total_reward: 36.77147001434825\n",
      "episode: 35, total_reward: -154.71320907180515\n",
      "episode: 36, total_reward: -166.9896187931535\n",
      "episode: 37, total_reward: 2.343570359626433\n",
      "episode: 38, total_reward: -23.474958567588036\n",
      "episode: 39, total_reward: -297.95866989405704\n",
      "episode: 40, total_reward: -55.05871320435374\n",
      "episode: 41, total_reward: -134.47832160746\n",
      "episode: 42, total_reward: -130.73789439719414\n",
      "episode: 43, total_reward: -73.89233258265793\n",
      "episode: 44, total_reward: -57.59753247968473\n",
      "episode: 45, total_reward: -97.74920943897537\n",
      "episode: 46, total_reward: -182.5505389012395\n",
      "episode: 47, total_reward: -110.99181339938258\n",
      "episode: 48, total_reward: -116.6926473470413\n",
      "episode: 49, total_reward: 5.472574307770909\n",
      "episode: 50, total_reward: 2.5497261823981283\n",
      "episode: 51, total_reward: -137.02523028635943\n",
      "episode: 52, total_reward: -76.74974075494147\n",
      "episode: 53, total_reward: -180.61096177188782\n",
      "episode: 54, total_reward: 44.63941220393791\n",
      "episode: 55, total_reward: -1.1577359755701593\n",
      "episode: 56, total_reward: -137.51895138013185\n",
      "episode: 57, total_reward: -135.82967223161722\n",
      "episode: 58, total_reward: -130.92224124564333\n",
      "episode: 59, total_reward: -220.78277642912366\n",
      "episode: 60, total_reward: -214.39185464400583\n",
      "episode: 61, total_reward: -105.45635709579483\n",
      "episode: 62, total_reward: -194.4100458811194\n",
      "episode: 63, total_reward: -182.83436382163433\n",
      "episode: 64, total_reward: -94.30558712355199\n",
      "episode: 65, total_reward: -4.991704284641826\n",
      "episode: 66, total_reward: 33.87883875007839\n",
      "episode: 67, total_reward: -63.64460291065792\n",
      "episode: 68, total_reward: -90.50629408431277\n",
      "episode: 69, total_reward: -245.63081896597382\n",
      "episode: 70, total_reward: -162.48045362732387\n",
      "episode: 71, total_reward: -17.53453752620411\n",
      "episode: 72, total_reward: -33.69231193356523\n",
      "episode: 73, total_reward: 15.713692938328377\n",
      "episode: 74, total_reward: -42.21080440187353\n",
      "episode: 75, total_reward: 16.18826655709292\n",
      "episode: 76, total_reward: -36.06897642909988\n",
      "episode: 77, total_reward: 2.785517319139199\n",
      "episode: 78, total_reward: 13.385318439924644\n",
      "episode: 79, total_reward: -10.350407273899865\n",
      "episode: 80, total_reward: -32.26383976007904\n",
      "episode: 81, total_reward: -34.76790479835538\n",
      "episode: 82, total_reward: -19.210849691072166\n",
      "episode: 83, total_reward: -13.04322045676403\n",
      "episode: 84, total_reward: -33.41671100603213\n",
      "episode: 85, total_reward: -15.692068033525677\n",
      "episode: 86, total_reward: 28.99427295370517\n",
      "episode: 87, total_reward: 3.251901978753442\n",
      "episode: 88, total_reward: 61.457172417399335\n",
      "episode: 89, total_reward: -10.987082746237473\n",
      "episode: 90, total_reward: 10.414582873907511\n",
      "episode: 91, total_reward: -222.41443786594007\n",
      "episode: 92, total_reward: -26.153294716710896\n",
      "episode: 93, total_reward: -6.017412396813909\n",
      "episode: 94, total_reward: -191.2185570872419\n",
      "episode: 95, total_reward: -242.56928392417151\n",
      "episode: 96, total_reward: 12.795606198600154\n",
      "episode: 97, total_reward: -37.90341835273699\n",
      "episode: 98, total_reward: -15.771695597509675\n",
      "episode: 99, total_reward: 62.937294608485274\n",
      "episode: 100, total_reward: 56.39423896500615\n",
      "episode: 101, total_reward: 15.67827752444196\n",
      "episode: 102, total_reward: 39.78492944185849\n",
      "episode: 103, total_reward: 30.39552433193852\n",
      "episode: 104, total_reward: 10.818894787763035\n",
      "episode: 105, total_reward: -32.979805342225404\n",
      "episode: 106, total_reward: 70.01353439144768\n",
      "episode: 107, total_reward: 73.489888788815\n",
      "episode: 108, total_reward: 25.972623850876307\n",
      "episode: 109, total_reward: 32.385933253819374\n",
      "episode: 110, total_reward: 45.83811278239819\n",
      "episode: 111, total_reward: 15.535236180148914\n",
      "episode: 112, total_reward: 16.984111103848427\n",
      "episode: 113, total_reward: 28.575529561233342\n",
      "episode: 114, total_reward: -81.76432074769527\n",
      "episode: 115, total_reward: 34.161786678008696\n",
      "episode: 116, total_reward: 27.7533528787538\n",
      "episode: 117, total_reward: -14.988145852440848\n",
      "episode: 118, total_reward: 36.52343117487188\n",
      "episode: 119, total_reward: 31.474044394067093\n",
      "episode: 120, total_reward: 31.401833374665088\n",
      "episode: 121, total_reward: 39.00132435313541\n",
      "episode: 122, total_reward: 40.816453529305\n",
      "episode: 123, total_reward: 60.63990861463755\n",
      "episode: 124, total_reward: 55.25193157413627\n",
      "episode: 125, total_reward: 57.43864384356151\n",
      "episode: 126, total_reward: 68.19401365940224\n",
      "episode: 127, total_reward: -8.952446149480958\n",
      "episode: 128, total_reward: 17.214155451097916\n",
      "episode: 129, total_reward: 18.02758760391798\n",
      "episode: 130, total_reward: 10.411556106829757\n",
      "episode: 131, total_reward: 9.290218409097811\n",
      "episode: 132, total_reward: -11.834313370715002\n",
      "episode: 133, total_reward: -39.12725432575627\n",
      "episode: 134, total_reward: 44.21559287179624\n",
      "episode: 135, total_reward: 20.28268354347356\n",
      "episode: 136, total_reward: 24.015715830386913\n",
      "episode: 137, total_reward: 60.909174556323364\n",
      "episode: 138, total_reward: 41.47194048854573\n",
      "episode: 139, total_reward: 55.37003900331027\n",
      "episode: 140, total_reward: 70.39831670274572\n",
      "episode: 141, total_reward: 87.5130387313428\n",
      "episode: 142, total_reward: 95.22025664457357\n",
      "episode: 143, total_reward: 21.083956441336024\n",
      "episode: 144, total_reward: 23.25073122934144\n",
      "episode: 145, total_reward: 8.473860907246895\n",
      "episode: 146, total_reward: 78.25456037306242\n",
      "episode: 147, total_reward: 11.445438749985005\n",
      "episode: 148, total_reward: 3.643363450867229\n",
      "episode: 149, total_reward: 51.73570975239049\n",
      "episode: 150, total_reward: -20.01225300293898\n",
      "episode: 151, total_reward: 40.9549413237323\n",
      "episode: 152, total_reward: 38.289557954911835\n",
      "episode: 153, total_reward: 26.207871571109973\n",
      "episode: 154, total_reward: 52.13456396874089\n",
      "episode: 155, total_reward: 25.689601441775174\n",
      "episode: 156, total_reward: 41.808110751217676\n",
      "episode: 157, total_reward: 30.76251461381825\n",
      "episode: 158, total_reward: -8.951924764864327\n",
      "episode: 159, total_reward: 74.32638351235111\n",
      "episode: 160, total_reward: 78.80872860396934\n",
      "episode: 161, total_reward: 54.04576591362719\n",
      "episode: 162, total_reward: 75.45755419603816\n",
      "episode: 163, total_reward: 31.898618864433015\n",
      "episode: 164, total_reward: -88.71669022199674\n",
      "episode: 165, total_reward: 68.48410597401272\n",
      "episode: 166, total_reward: 82.41940946572346\n",
      "episode: 167, total_reward: -3.5514189933251687\n",
      "episode: 168, total_reward: 225.10574130920793\n",
      "episode: 169, total_reward: 31.872615909271854\n",
      "episode: 170, total_reward: 70.19674063329234\n",
      "episode: 171, total_reward: 87.03237155696608\n",
      "episode: 172, total_reward: 60.3147955555823\n",
      "episode: 173, total_reward: 73.49650514033848\n",
      "episode: 174, total_reward: 11.80946358588021\n",
      "episode: 175, total_reward: 41.18704041238741\n",
      "episode: 176, total_reward: 58.48950031137595\n",
      "episode: 177, total_reward: 12.711013686220547\n",
      "episode: 178, total_reward: 72.74593620408687\n",
      "episode: 179, total_reward: 26.40545936814795\n",
      "episode: 180, total_reward: 85.92059991835053\n",
      "episode: 181, total_reward: 62.95050843722944\n",
      "episode: 182, total_reward: 16.091337956515517\n",
      "episode: 183, total_reward: 8.94454952700157\n",
      "episode: 184, total_reward: 29.221991555044255\n",
      "episode: 185, total_reward: 29.311964562271065\n",
      "episode: 186, total_reward: 51.648831822350104\n",
      "episode: 187, total_reward: -7.856414699024579\n",
      "episode: 188, total_reward: 53.721900651845935\n",
      "episode: 189, total_reward: 17.041873888584238\n",
      "episode: 190, total_reward: -48.66843557007094\n",
      "episode: 191, total_reward: 48.58157042160253\n",
      "episode: 192, total_reward: 66.16734796850419\n",
      "episode: 193, total_reward: 61.41345217491509\n",
      "episode: 194, total_reward: 21.78201936863082\n",
      "episode: 195, total_reward: 38.945735264427185\n",
      "episode: 196, total_reward: 18.86091096765748\n",
      "episode: 197, total_reward: 37.143806232637424\n",
      "episode: 198, total_reward: 53.789286507674305\n",
      "episode: 199, total_reward: 40.37891621421554\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = DQN(state_dim, action_dim)\n",
    "\n",
    "episode_n = 200\n",
    "t_max = 500\n",
    "\n",
    "for episode in range(episode_n):\n",
    "    total_reward = 0\n",
    "\n",
    "    state = env.reset()\n",
    "    for t in range(t_max):\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        \n",
    "        agent.fit(state, action, reward, done, next_state)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f'episode: {episode}, total_reward: {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
