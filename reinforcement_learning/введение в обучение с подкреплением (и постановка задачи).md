## Обучение с подкреплением
Обучение с подкреплением - это обучение модели действиям, которые максимизируют численный сигнал-вознаграждение. В большинстве сложных задач действия необходимо рассматривать действия не только как влияющие на непосредственное вознаграждение, но и на следующую ситуацию, а значит на все последующие вознаграждения. Задачу обучения с подкреплением можно формализовать как задачу оптимального управления марковским процессом принятия решений. Обучающийся агент должен уметь в какой-то степени воспринимать состояние среды и предпринимать действия, изменяющие его состояние, с целью достижения поставленной цели. Важная черта обучения с подкреплением – явное рассмотрение целостной проблемы целеустремленного агента, взаимодействующего с неопределенной окружающей средой. При этом под агентом понимается программный математический модуль, который будет принимать решения, выполнять действия и на основании ответов среды обучаться. Сторона, генерирующая вознаграждения за действия агента, называется окружающей средой. Генерируемые средой вознаграждения определяют цель агента – максимизация полного вознаграждения (сумма вознаграждений за длительный период времени). Для достижения этой цели агенту необходимо сформировать оптимальную стратегию своих действий, основанную на итерационном выполнении действий и сохранении вектора полученных вознаграждений в ответ на определенные события. Формирование оптимальной стратегии и есть основная задача обучения с подкреплением, поскольку ее достаточно для определения наилучшего поведения агента в текущих условиях окружающей среды. Инструментом для решения задачи поиска оптимальной стратегии выступает функция ценности, которая оценивает возможность получить вознаграждение не только сейчас, но и в длительной перспективе.

### Математическая постановка задачи
Поскольку поведение агента в среде можно описать **марковским процессом принятия решения** введем **необходимые определения**:

* $S$ - множество состояний
* $A$ - множество действия
* $Pfunc$ - функция возвращающая веростность перехода в следующее состояние при выборе определенного действия в определенном
$Pfunc(s^{'}|s, a) = P[S_{t+1} = s^{'}| S_t = s, A_t = a]$
* $Pfunc_0$ - функция инициализации начального (нулевого) состояния
* $Rfunc$ - функция выдачи награды $Rfunc(s, a) = R_t \iff P[R_t|S_t = s, A_t = a] = 1$
* $\gamma$ - коэффициент дисконтирования (угасания ценности последующих наград относительно награды на следующем шаге)

и следующие марковские свойства:

$P[S_{t+1}|S_t, A_t] = P[S_{t+1}|S_0, A_0, S_1, A_1...S_t, A_t]$ - вероятность перехода агента в следующее состояние в которое агент перемещается после действия зависит только от текущего состояния и действия (не зависит от предыдущих состояний, действий, полученных наград и т.д.)

$P[R_t|S_t, A_t] = P[R_t|S_0, A_0, S_1, A_1...S_t, A_t] = 1$ - получаемая награда агента зависит от текущего состояния и действия и детерминирована, т.е. награда является постоянной величиной и за определенный переход в следующее состояние мы всегда будем получать одинаковое вознаграждение.

**Цель агента** - максимизация суммы с наград с учетом коэффициента дисконтирования $G = \sum^{\infty}_{t=0}\gamma^tR_t \rightarrow max$

или для марковского процесса с финальными состояниями $G = \sum^{T}_{t=0}\gamma^tR_t \rightarrow max$, где $S_T \in S_F$

При этом мы можем преобразовывать задачу от одной к другой, для преобразования задачи с финальными состояними к общей принимаем множество финальных состояний пустым, обратную задачу решаем определением состояний (по сути финальных) с вероятностью перехода в него же равной единице и нулевой наградой за такой переход.

### Интерфейс OpenAI Gym Interface
Функции:
* initial_state = env.reset()
    * unitial_state - инициализация начального состояния $S_0 \sim Pfunc_0$
    * env.state = initial_state
* next_state, reward, done, info = env.step(action)
    * action - текущее действие $A_t$
    * next_state - следующее состояние $S_{t+1} \sim Pfunc(S_{t+1}|S_t, A_t)$
    * reward - текущая награда $R_t = Rfunc(S_t, A_t)$
    * done - при true достигнуто финальное состояние $S_{t+1} \in S_F$
    * info - дополнительная информация
    * env.state = next_state
 
### Политика
Политика - это последовательность действий $\pi: S \rightarrow A$
* имеем множество политик $\pi$
* агент инициализирует начальное состояние $S_0 \sim Pfunc_0$
* совершает действие в соответствии с политикой $A_0 = \pi(S_0)$
* получает награду $R_0 = Rfunc(S_0, A_0)$ и переходит в следующее состояние $S_1 \sim Pfunc(S_1|S_0, A_0)$
* совершает следующее действие $A_1 = \pi(S_1)$
* получает награду $R_1 = Rfunc(S_1, A_1)$ и так далее
* ...
* получаем конечную сумму наград (ценность) $G = \sum^{\infty}_{t=0}\gamma^tR_t$

Соответственно задача обучения с подкреплением сводится к выбору оптимальной стратегии максимизирующей математическое ожидание ценности $\mathbb{E}_{\pi}[G] \rightarrow \underset{\pi}{max}$

**Стохастическая политика** - политика при котором в состояние выдается не действие, а вероятностное распределение действий $\pi(a|s) \in [0, 1]$

При действии в соответствии с политикой получаем **траекторию** $\tau = \{S_0, A_0, S_1, A_1, S_2, A_2 ...\}$

Соответственно вероятность получить траекторию $\tau$ при политике $\pi$ является произведением всех вероятностей перехода $\mathbb{P}(\tau, \pi) = \prod_{t=0}^{\infty} \pi(A_t|S_t)Pfunc(S_{t+1}|S_t, A_t)$

А математическое ожидание ценности равно сумме ценностей с учетом вероятностей траекторий $\mathbb{E}_{\pi}[G] = \sum_{\tau} G(\tau)\mathbb{P}(\tau, \pi)$ или интрегралу при непрерывных траекториях $\mathbb{E}_{\pi}[G] = \int_{\tau} G(\tau)\mathbb{P}(\tau, \pi)$

### Расчет матожидания ценности $\mathbb{E}_{\pi}[G]$
Ценность приблизительно равна сумме ценностей семплированный K стратегий при K стремящемся к $\infty$
$$\mathbb{E}_{\pi}[G] \approx \frac{1}{K} \sum_{k=1}^K G(\tau_k), \tau_k \sim \{P, P_0, \pi\}$$

Соответственно расчет ценности представляет собой достаточно сложный процесс вычисления, а с учетом оптимизации по стратегиям становится еще сложнее. Для решения поставленной задачи используются различные алгоритмы обучения с подкреплением, которые рассмотрены отдельными файлами.

#machine_learning #reinforcement_learning 
