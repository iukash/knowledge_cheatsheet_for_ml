### [Строки](https://pythoninfo.ru/osnovy/str-python)
* [upper()](https://docs-python.ru/tutorial/operatsii-tekstovymi-strokami-str-python/metod-str-upper/) - преобразует строку к верхнему регистру `str.upper()`
* [lower()](https://docs-python.ru/tutorial/operatsii-tekstovymi-strokami-str-python/metod-str-lower/) - преобразует строку к нижнему регистру `str.lower()`
* [find()](https://docs-python.ru/tutorial/operatsii-tekstovymi-strokami-str-python/metod-str-find/) - поиск подстроки, вернет индекс начала подстроки либо -1 в случае отсутствия `str.find(start_index, end_index)`
* [replace()](https://docs-python.ru/tutorial/operatsii-tekstovymi-strokami-str-python/metod-str-replace/) - замена подстроки аргумента в строке `str.replace('str_old', 'str_replace')`
* [split()](https://docs-python.ru/tutorial/operatsii-tekstovymi-strokami-str-python/metod-str-split/) - делит строку на список по разделителю `str.split(sep='/n', maxsplit=-1)`
* [форматирование строк](https://practicum.yandex.ru/learn/data-scientist/courses/d40b492e-2aa3-4d6a-b51d-9f442463e9a2/sprints/16528/topics/a9499478-7fe5-4f86-8beb-58c753443495/lessons/5ae8e887-fe12-4773-8747-626e999567b3/)
    * без f-строк `word 1: {0}, word 2: {1}, word 3: {2}'.format(word1, word2, word3)`
    * с f-строками `f'word 1: {word1}, word 2: {word2}, word 3: {word3}'`
* [стемминг](https://practicum.yandex.ru/learn/data-scientist/courses/6ecee377-0b07-4799-b0f6-1a87cdf90810/sprints/18599/topics/8d9cd355-9a61-4d8a-8668-54002204d348/lessons/786398f5-7565-482d-9efb-7c2d5bec8b32/) (выделение основы слова)
* [лемматизация](https://practicum.yandex.ru/learn/data-scientist/courses/6ecee377-0b07-4799-b0f6-1a87cdf90810/sprints/18599/topics/8d9cd355-9a61-4d8a-8668-54002204d348/lessons/0cf86bb8-0f6e-4224-98d0-522c0b8f9256/) - приведение слова к словарной форме
 * `from pymystem3 import Mystem`
 * `m = Mystem()`
 * `m.lemmatize(text)`
* **Получение корпуса в кодировке Юникод** `data['text'].values.astype('U')`
* [re.sub()](https://docs-python.ru/standart-library/modul-re-python/funktsija-sub-modulja-re/) - Поиск в тексте всех совпадений по шаблону и замена их заданной строкой `re.sub(pattern, replacement, text)`
* [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) - Получение мешка слов 
 * `from sklearn.feature_extraction.text import CountVectorizer`
 * `count_vect = CountVectorizer(stop_words=stopwords) # stopwords - список стоп-слов`
 * `bow = count_vect.fit_transform(corpus) # bow, от англ. bag of words`
 * `words = count_vect.get_feature_names() # словарь уникальных слов`
* [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) - Получение списка N-грамм 
 * `n from sklearn.feature_extraction.text import CountVectorizer`
 * `count_vect = CountVectorizer(ngram_range=(min_n, max_n))`
* **stopwords** - Получение стоп-слов для русского языка 
 *`import nltk`
 * `from nltk.corpus import stopwords`
 * `nltk.download('stopwords')`
 * `stopwords = set(stopwords.words('russian'))`
* [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) - Получение TF-IDF для корпуса текста 
 * `from sklearn.feature_extraction.text import TfidfVectorizer`
 * `count_tf_idf = TfidfVectorizer(stop_words=stopwords) # stopwords - список стоп-слов`
 * `tf_idf = count_tf_idf.fit_transform(corpus)`


#machine_learning #preprocessing 
